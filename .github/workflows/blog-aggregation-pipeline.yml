name: Blog Aggregation Pipeline

on:
  # Run every 6 hours
  schedule:
    - cron: '0 */6 * * *'
  
  # Allow manual trigger
  workflow_dispatch:
    inputs:
      use_database:
        description: 'Write to PostgreSQL database'
        required: false
        default: 'false'
        type: choice
        options:
          - 'true'
          - 'false'

  # Run on push to pipeline files
  push:
    branches: [main, develop, phase3to5]
    paths:
      - 'services/beam-pipelines/**'
      - '.github/workflows/blog-aggregation-pipeline.yml'

env:
  PYTHON_VERSION: '3.11'

jobs:
  # ============================================
  # Run Blog Aggregation Pipeline
  # ============================================
  run-pipeline:
    name: ðŸ“° Blog Aggregation
    runs-on: ubuntu-latest
    
    services:
      postgres:
        image: postgres:15
        env:
          POSTGRES_USER: blog_user
          POSTGRES_PASSWORD: blog_password
          POSTGRES_DB: blog_db
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 5432:5432

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'

      - name: Install dependencies
        working-directory: ./services/beam-pipelines
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: Create output directory
        run: mkdir -p services/beam-pipelines/output

      - name: Run Pipeline (File Output)
        if: github.event_name == 'push' || (github.event_name == 'workflow_dispatch' && github.event.inputs.use_database == 'false')
        working-directory: ./services/beam-pipelines
        run: |
          python -m blog_aggregation.main \
            --runner=DirectRunner \
            --use_database=false \
            --output_path=output/blog_articles

      - name: Run Pipeline (Database Output)
        if: github.event_name == 'schedule' || (github.event_name == 'workflow_dispatch' && github.event.inputs.use_database == 'true')
        working-directory: ./services/beam-pipelines
        env:
          DATABASE_URL: postgresql://blog_user:blog_password@localhost:5432/blog_db
        run: |
          python -m blog_aggregation.main \
            --runner=DirectRunner \
            --use_database=true \
            --database_url="${DATABASE_URL}"

      - name: Upload Results (File Output)
        if: github.event_name == 'push' || (github.event_name == 'workflow_dispatch' && github.event.inputs.use_database == 'false')
        uses: actions/upload-artifact@v4
        with:
          name: blog-articles-${{ github.run_number }}
          path: services/beam-pipelines/output/
          retention-days: 7
          if-no-files-found: ignore

      - name: Verify Database (if used)
        if: github.event_name == 'schedule' || (github.event_name == 'workflow_dispatch' && github.event.inputs.use_database == 'true')
        env:
          PGPASSWORD: blog_password
        run: |
          echo "ðŸ“Š Article Count in Database:"
          psql -h localhost -U blog_user -d blog_db -c \
            "SELECT COUNT(*) as total_articles FROM aggregated_blog_articles;"
          
          echo "ðŸ“° Latest Articles:"
          psql -h localhost -U blog_user -d blog_db -c \
            "SELECT source_name, title, published_at FROM aggregated_blog_articles ORDER BY created_at DESC LIMIT 10;"

  # ============================================
  # Lint and Test Pipeline Code
  # ============================================
  lint-and-test:
    name: ðŸ§ª Lint & Test
    runs-on: ubuntu-latest
    if: github.event_name == 'push'
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'

      - name: Install dependencies
        working-directory: ./services/beam-pipelines
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install flake8 pytest

      - name: Lint with flake8
        working-directory: ./services/beam-pipelines
        run: |
          flake8 . --count --select=E9,F63,F7,F82 --show-source --statistics || true
          flake8 . --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics

      - name: Run unit tests
        working-directory: ./services/beam-pipelines
        run: |
          pytest -v || echo "No tests found, skipping"
        continue-on-error: true

  # ============================================
  # Summary Job
  # ============================================
  pipeline-summary:
    name: ðŸ“Š Pipeline Summary
    runs-on: ubuntu-latest
    needs: [run-pipeline]
    if: always()
    
    steps:
      - name: Display summary
        run: |
          echo "## ðŸ“° Blog Aggregation Pipeline Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "| Detail | Value |" >> $GITHUB_STEP_SUMMARY
          echo "|--------|-------|" >> $GITHUB_STEP_SUMMARY
          echo "| Trigger | ${{ github.event_name }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Pipeline Status | ${{ needs.run-pipeline.result }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Branch | ${{ github.ref_name }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Run Number | ${{ github.run_number }} |" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          if [ "${{ github.event_name }}" == "schedule" ]; then
            echo "â° **Scheduled Run** - Every 6 hours" >> $GITHUB_STEP_SUMMARY
          elif [ "${{ github.event_name }}" == "workflow_dispatch" ]; then
            echo "ðŸ”§ **Manual Run** - Database: ${{ github.event.inputs.use_database }}" >> $GITHUB_STEP_SUMMARY
          else
            echo "ðŸ”„ **Push Triggered**" >> $GITHUB_STEP_SUMMARY
          fi
